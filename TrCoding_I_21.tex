\input{preamble.tex}
\DeclareMathOperator{\cwd}{codeword}
\newtheorem{proposition}{Proposition}
\usepackage{forest}
\usepackage{lipsum}
\usepackage{subcaption}
\usepackage{mathtools}
\DeclareMathOperator{\dist}{dist}
\DeclareMathOperator{\dimension}{dim}
\DeclareMathOperator{\gradient}{grad}
\begin{document}
\section{Transform Coding I} 
\begin{frame}
 \vspace{12.0ex}
\begin{center}
\begin{beamercolorbox}[sep=12pt,center]{part title}
\usebeamerfont{section title}\insertsection\par
\end{beamercolorbox}
\end{center}
\end{frame}
\section{Introduction}
\begin{frame}{Review of last lecture}
\loud{Zador Formula:}
\bit
\item  Generalization of Panter and Dite approximation to higher 
dimesion. 
\item Given an $N$-dimensional source $X$ with suitable regular density $f_X$. 
\item The distortion rate function of fixed length vector quantization for $X$  asymptotically equals 
\begin{align}\label{PanterDiteGen}
D_{F,N}(R)=C(N)\left(\int_{\mathbb{R}^N}f(x)^{\frac{N}{N+2}}dx\right)^{\frac{N+2}{N}}2^{-2R}, 
\end{align}
as the rate $R$ tends to $\infty$. 
\item Constant $C(N)$ is \loud{independent of the source}. 
\item Deep research work and open conjectures on the description of $C(N)$. 
\item Asymptotic behaviour: 
\begin{align}\label{CNasymptotic}
\lim_{N\to\infty}C(N)=\frac{1}{2\pi e}.
\end{align}
\eit
\end{frame}


\begin{frame}{Space filling advantage of vector quantization} 
\bit
\item Source-independent advantage of vector quantization over scalar quantization. 
\item Related to better packing property of more complicated vector quantizers in comparison to scalar quantizers. 
\item Quantified by the quotient 
\begin{align*}
\frac{C(1)}{C(N)}. 
\end{align*}
\item \loud{Not} possible to design any compression system invoking scalar quantization that can exploit the space filling advantage of vector quantization. 
\item General form of vector quantization very complex. 
\item Very large memory requirement for reconstruction points at encoder and decoder, very complicated search algorithm at encoder. 
\item But: Low-complexity variants like \loud{lattice quantization} or \loud{trellis coded quantization} exist. 
\eit
\end{frame}


 \input{TrCoding/SpaceFillingExample_New.tex}

\begin{frame}{Shape and memory advantage}
\bit
\item Given an $\mathbb{R}^N$-valued source $X=(X_1,\dots,X_N)$ with distribution $p_X$. 
\item Let $p_{X_i}$ denote the marginal distribution of the $i$-th component $X_i$ with  density $\tilde{f}$.  
\item Assume $X$ is stationary, in particular, marginal probabilities satisfy $p_{X_i}=p_{X_j}$ for all $i, j$. 
\item \loud{Shape and memory advantage: } Compare source-dependent terms in Panter and Dite formula for 1-dimensional and $N$-dimensional case invoking density $f$ and 
density $\tilde{f}$.
\eit 
\vspace{-0.1em}
Shape and memory advantage depend on source $X$. \loud{\iarrow$ $ Idea of transform coding: }
\bit
\item Modify source $X$, code $\phi(X)$ instead of $X$ . Use scalar quantizers for $\phi(X)$. 
\item A priori, $\phi$ can be \textit{any} deterministic mapping that is invertible with inverse $\phi^{-1}$. 
\item Distribution of  individual components of $\phi(X)$ is different than for $X$.
\item Design goal: Reduce impact of shape and memory advantage for $\phi(X)$ compared to original $X$.
\item More informally: Try to achieve \loud{energy compaction} by concentrating main information contained in $\phi(X)$ in its first few components.
\item \ALERT{Transform coding}: Mapping $\phi$ is linear, $\phi$ is a \loud{transform}.   
\eit
\end{frame}





\section{The Karhuenen Loewe Transform} 
\begin{frame}
 \vspace{12.0ex}
\begin{center}
\begin{beamercolorbox}[sep=12pt,center]{part title}
\usebeamerfont{section title}\insertsection\par
\end{beamercolorbox}
\end{center}
\end{frame}

\begin{frame}\frametitle{Low rank approximation}

\loud{Goal of low rank approximation: }
\bit
\item Try to represent $X$ in a different coordinate system such that first components carry most meaningful information on $X$. 
\item[\iarrow]  For each $k\in\{1,\dots,N\}$, find \loud{best $k$-dimensional approximation of $X$.}
\eit
\loud{Formal definition:} 
\bit  
\item For each $k$, find $k$-dimensional subspace $V$ of $\mathbb{R}^N$ which is of minimal expected distance 
to $X$, i.e. which solves
\begin{align}\label{BestApproxMin}
\underset{V\colon \dimension(V)=k}{\min}\mathbb{E}(\dist(V,X)^2).
\end{align} 
\item Here, $\dist$ denotes the Euclidean distance:
\begin{align*}
dist(x,V)=\min_{v\in V}\left\|x-v\right\|, \quad x\in\mathbb{R}^N. 
\end{align*} 
\item Minimum in \eqref{BestApproxMin} is taken over all $k$-dimensional subspaces of $\mathbb{R}^N$.
\eit
\end{frame}


\begin{frame}\frametitle{Energy compaction}
\begin{itemize}
\item If $V$ is a $k$-dimensional subspace of $\mathbb{R}^N$, let $v_1,\dots,v_N$ be an orthonormal basis of $\mathbb{R}^N$ such 
that $v_1,\dots,v_k$ is an orthonormal basis of $V$. Then:
\begin{align}\label{normminusdist}
\mathbb{E}(\dist(V,X)^2)=&\sum_{i=k+1}^N\mathbb{E}\left(<X,v_i>^2\right)\nonumber\\
=&\mathbb{E}(\left\|X\right\|^2)-\sum_{i=1}^k\mathbb{E}\left(<X,v_i>^2\right).
\end{align}
\item By \eqref{normminusdist}, $V$ minimizes \eqref{BestApproxMin} if and only if each orthonormal basis $v_1,\dots,v_k$ of $V$ solves
\begin{align}\label{BestApproxMax}
\underset{\underset{<v_i,v_j>=\delta_{i,j} \colon \forall i,j\in\{1,\dots,k\}}{\{v_1,\dots,v_k\}}}{\max}\mathbb{E}(<X,v_1>^2)+\dots+\mathbb{E}(<X,v_k>^2).
\end{align} 
Here $\delta_{i,j}=1$ if $i=j$ and $\delta_{i,j}=0$, else.
\item An orthonomal set which solves \eqref{BestApproxMax} \loud{optimally concentrates or compacts the energy of $X$ in $k$ components}. 
\item[\iarrow] \loud{Low rank approximation and energy compaction are equivalent problems.}
\end{itemize}
\end{frame}


\begin{frame}\frametitle{Autocorrelation matrix} 
\loud{How does one achieve energy compaction:} 
\bit
\item Assume from now that $X$ is of \loud{zero mean}. 
\item The matrix 
\begin{align*}
R_X:=\mathbb{E}(XX^t)
\end{align*}
is called \loud{autocorrelation matrix }of $X$.
\eit

\loud{Properties of the autorcorrelation matrix $R_X$:}
\bit
\item The $i$-th diagonal element $R_X(i,i)$ of $R_X$ is the variance $\sigma_i^2(X)$
of $X_i$. 
\item $R_X$ is symmetric.
\item For any $v\in\mathbb{R}^N$ one has 
\begin{align*}
v^tR_Xv=\mathbb{E}(<v,X>^2)\geq 0.
\end{align*}
In particular, $R_X$ is positive semidefinite. 
\item For an $N\times N$-Matrix $T$, one has 
\begin{align*}
R_{TX}=TR_XT^t. 
\end{align*}
\eit
\end{frame}

\begin{frame} \frametitle{The Karhuenen Loeve Transform (KLT)}
\loud{Definition of a KLT:}
\bit
\item An orthogonal matrix $T$ for which 
$R_{TX}=TR_{X}T^t$ is a diagonal matrix is called a KLT for $X$. 
\item Since $R_X$ is symmetric, it is diagonalizable and there exists an orthonormal basis of eigenvectors. 
\item [\iarrow] A KLT always exists.
\eit
\loud{Basic properties of KLTs:}
\bit 
\item The rows of the KLT comprise an orthonormal basis of eigenvectors of $R_X$. 
\item If the eigenvalues of $R_X$ are all distinct, a KLT is unique up to permutations and multiplications by $\pm 1$ of its rows. 
\item No general algorithm to exactly compute the eigenvalues of $R_X$ exists.
\item Well working iterative schemes for compuation of KLT which are convergent:
\bit 
\item QR-method 
\item Jacobi method
\eit
\eit
\end{frame}

\begin{frame}\frametitle{KLT yields optimal energy compaction}
\begin{proposition}[Optimal Energy Compaction of the KLT]\label{TheoremKLTEnergy}
Let $\lambda_1,\dots,\lambda_N$ denote the eigenvalues of $R_X$, ordered such that 
\begin{align*}
\lambda_1\geq \lambda_2\geq\dots\geq \lambda_N\geq 0
\end{align*}
and let be $\{v_1,\dots,v_N\}$ be an associated orthonormal basis of eigenvectors.
\bit
\item For every $k\in\{1,\dots,N\}$, the set $\{v_1,\dots,v_k\}$ solves  \eqref{BestApproxMax}. 
\item Equivalently, for 
every $k\in\{1,\dots,N\}$, the space 
\begin{align*}
V_k:=\mathbb{R}v_1\oplus\cdots\oplus\mathbb{R}v_k
\end{align*}
solves \eqref{BestApproxMin}.
\eit
\end{proposition}
\loud{\iarrow For a given source $X$, KLT computs best low-rank approximation of $X$.}
\end{frame}

\begin{frame}\frametitle{Proof of optimal energy compaction of KLT for $k=1$.}
\begin{itemize}
\item For $k=1$, one needs to finde $v_1\in\mathbb{R}^N$ which solves the constraint optimization problem 
\begin{align}\label{maximization}
 & \text{maximize}\quad &\mathbb{E}(<X,v>^2) \nonumber \\ & \text{subject to}\quad &\left\|v\right\|^2=1.
\end{align}
\item One has 
\begin{align}\label{ExpValSP}
\mathbb{E}(<X,v>^2)=\mathbb{E}(v^tXX^tv)
=v^tR_Xv.
\end{align}
\item By the Lagrangian multiplier law, $v_1$ satisfies  
\begin{align*}
&\gradient\mathbb{E}(<X,v_1>^2) =\lambda \gradient\left\|v_1\right\|^2 
\iff &2R_Xv_1=2\lambda v_1
\end{align*}
for some $\lambda\in\mathbb{R}$. 
\item Thus, $v_1$ needs to be an eigenvector of $R_X$ of eigenvalue $\lambda$. By \eqref{ExpValSP}, one 
has 
\begin{align*}
\mathbb{E}(<X,v>^2)=\lambda^2
\end{align*}
and thus \eqref{maximization} is maximized if $\lambda=\lambda_1$. 
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Proof of optimal energy compaction of KLT  for general $k$, I }
\begin{itemize}
\item Perform induction on k. Assume that the statement holds for $v_1,\dots,v_{k-1}$. 

\item Let $v_k$ be a solution of the constraint optimization problem 
\begin{align}\label{maximizationGen}
 & \text{maximize} &\mathbb{E}(<X,v>^2) \nonumber \\ & \text{subject to}\: &\left\|v\right\|^2=1\: \text{and}<v,v_i>=0, \:\forall i\in\{1,\dots,k-1\}.
\end{align}
\item Again, by the Lagrangian multiplier law there exist $\lambda,\:\mu_1,\dots,\mu_{k-1}\in\mathbb{R}$ such that
\begin{align*}
2R_Xv_k=2\lambda v_k+\mu_1v_1+\dots+\mu_{k-1}v_{k-1}. 
\end{align*}
\item Since $R_Xv_i=\lambda_i$, taking the scalar product with $v_i$ implies 
\begin{align*}
0=2\lambda_iv_i^tv_k=(2R_Xv_i)^tv_k=2v_i^tR_Xv_k=\mu_i.
\end{align*}
for each $i\in\{1,\dots,k-1\}$. 
\item Thus $v_k$ is an eigenvector of $R_X$ with eigenvalue $\lambda$. Maximizing \eqref{maximizationGen} implies 
$\lambda=\lambda_k$. 
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Proof of optimal energy compaction of KLT  for general $k$, II}
\begin{itemize}
\item Let $W$ be an arbitrary $k$-dimensional subspace of $\mathbb{R}^N$. Then $W$ contains a vector $w$ which is orthogonal to $v_1,\dots,v_{k-1}$. 
\item Choose vectors $w_1,\dots,w_{k-1}$ such that $\{w_1,\dots,w_{k-1},w\}$ is an orthonormal basis of $W$. 
\item By induction hypothesis and 
by the property of $v_k$ one has 
\begin{align*}
&\sum_{i=1}^{k-1}\mathbb{E}(<X,w_i>^2)+\mathbb{E}(<X,w>^2)\\
\leq& \sum_{i=1}^{k-1}\mathbb{E}(<X,v_i>^2)+\mathbb{E}(<X,v_k>^2). 
\end{align*}
\item By \eqref{normminusdist}, this implies
\begin{align*}
\mathbb{E}(\dist(X,W)^2)\geq \mathbb{E}(\dist(X,V_k)^2).
\end{align*}
\end{itemize}
\qed
\end{frame}

\section{Principal Component Analysis}

\begin{frame}
 \vspace{12.0ex}
\begin{center}
\begin{beamercolorbox}[sep=12pt,center]{part title}
\usebeamerfont{section title}\insertsection\par
\end{beamercolorbox}
\end{center}
\end{frame}




\begin{frame}\frametitle{KLT and Principal Component Analysis}
\loud{Definition of principal component analysis} 
\begin{itemize}
\item Let $\lambda_1\geq\dots\geq \lambda_N$ be the eigenvalues of $R_X$ and let $T$ be a KLT for $X$. 
\item Reorder 
rows of $T$ such that $i$-th row is an eigenvector of $T$ of eigenvalue $\lambda_i$.
\item The $i$-th row of $T$ is called the i-th \loud{principal component} of $X$. 
\item Consider the random variable 
\begin{align*}
Y=TX=\begin{pmatrix}Y_1\\ \vdots\\Y_N\end{pmatrix}
\end{align*}
instead of $X$
\item First few components of $Y$ carry the most meaningful information 
about $X$.  
\item Given a realization $x\in\mathbb{R}^N$ of $X$, computing the first $k$ components $y_1,\dots,y_k$ of $Tx$,
where often $k\ll N$,  is called principal component analysis (PCA). 
\end{itemize}
\end{frame}


\begin{frame}\frametitle{PCA for analysis of large data sets}
\begin{itemize}
\item Random variable $X$ is often derived as an empirical distribution.
\item Assume that $d$ measurements $x_d\in\mathbb{R}^N$ of $N$ variables are conducted where typically $d\gg N$. 
Let $x_{i,j}$ be the value of the $j$-th variable at the $i$-th measurement. 
For $j\in\{1,\dots,N\}$ let 
\begin{align*}
\mu_j:=\frac{1}{d}\sum_{i=1}^dx_{i,j}
\end{align*}
denote the $j$-th empirical mean and let $\overline{x}_{i,j}:=x_{i,j}-\mu_j$ be the 
mean-corrected measurements. 
\item Then the $d\times N$ matrix whose 
$i$-th row and $j$-th column is given by $\overline{x}_{i,j}$ is often called (normalized) data matrix $M_{data}$.
\item The auto-covariance matrix of the empirical distribution $X$ defined by the $\overline{x}_{i,j}$ is 
given by 
\begin{align*}
R_X=M_{data}^tM_{data}. 
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}\frametitle{Example of a PCA}
Taken from Bishop, Pattern Recognition and Machine Learning 
\begin{itemize}
\item Consider a dataset that consists of a large collection of handwritten digits of the number $3$, taken from MNIST dataset.
\item Each data-sample of an handwritten digit is a grey-scale image with $28\times 28$ pixels. 
\item Thus: Data samples correspond to vectors $x_d\in \mathbb {R}^N$ for $N=748$. 
\end{itemize}
\begin{figure}
\centering
\includegraphics[width=0.95\textwidth]{TrCoding/number_3_MNIST.png}
\captionsetup{labelformat=empty}
\caption{Example for handwritten digits from MNIST.}
\end{figure}
\end{frame}

\begin{frame}{ Visualization of principal components} 
\begin{itemize}
\item PCA: Let $\lambda_1\geq\lambda_2\geq\cdots\geq \lambda_N$ denote the eigenvalues 
of the empirical covariance matrix, and $v_1,\dots,v_N$ be the assiocated eigenvectors, the principal components.
\end{itemize}
\begin{figure}
\includegraphics[width=0.5\textwidth]{TrCoding/FigurePCA_1.png}
\captionsetup{labelformat=empty}
\caption{The mean (blue) and the first four principal components $v_1,\dots,v_4$ (yellow) of the handwritten digits of the number 3. }
\end{figure}
\end{frame}

\begin{frame}{Visualization of energy compaction of PCA}
\begin{figure}
\includegraphics[width=0.75\textwidth]{TrCoding/FigurePCAEx.pdf}
\captionsetup{labelformat=empty}
\caption{Original handwritten digit (left), and its representation by using only a subset $v_1,\dots,v_M$ of the principal components. If $x\in\mathbb{R}^{748}$, the $M$-th picture 
represents
  \begin{minipage}{\linewidth}
     \begin{equation*}
        \mu+\sum_{i=1}^M<x,v_i>v_i,
   \end{equation*}
   \end{minipage}
where $\mu\in\mathbb{R}^N$ is the mean of the data. 
}
\end{figure}
\end{frame} 


\section{Definition of transform coding}


\begin{frame}
 \vspace{12.0ex}
\begin{center}
\begin{beamercolorbox}[sep=12pt,center]{part title}
\usebeamerfont{section title}\insertsection\par
\end{beamercolorbox}
\end{center}
\end{frame}

\begin{frame}\frametitle{Transform coding}

\begin{definition}[Transform coding]
Let $S$ be an $N$-dimensional source. Invertible $N\times N$-matrices $A, B$ and scalar quantizers $Q_1,\dots,Q_N$ 
define the transform coding of $s\in\mathbb{R}^N$ , realization of $S$, as the following ordered steps:
\begin{enumerate}
\item Compute $u=As$. The components $u_1,\dots,u_N$ of $u$ are called transform coeffients. 
\item Code each $u_i$ with $Q_i$ to obtain the reconstructed transform coefficients $u_i'$ which comprise the reconstructed transformed vector $u'$. 
\item Compute the reconstructed vector $s'$ as $s'=Bu'$.   
\end{enumerate}
\end{definition}
\bit
\item The matrix $A$ is called \loud{analysis transform}.
\item The matrix $B$ is called \loud{synthesis transform}. 
\item In most cases, one has $B=A^{-1}$. 
\eit
\end{frame}

\input{TrCoding/StructureTrCoding_New.tex}
\input{TrCoding/EncoderDecoder_New.tex}
\end{document}

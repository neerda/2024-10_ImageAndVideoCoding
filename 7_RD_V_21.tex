%\documentclass{beamer}
%\usepackage{amsmath}
%\usepackage{amsfonts}
%\usepackage{amsthm}
%\usepackage{amssymb}
%\usepackage{tikz}
%\usetikzlibrary{trees}
%\usepackage{lipsum}
\input{preamble.tex}
\DeclareMathOperator{\cwd}{codeword}
\newtheorem{proposition}{Proposition}
\usepackage{forest}
\usepackage{lipsum}
\usepackage{subcaption}
\usepackage{mathtools}
\begin{document}

\section{RD-theory V} 
\begin{frame}
 \vspace{12.0ex}
\begin{center}
\begin{beamercolorbox}[sep=12pt,center]{part title}
\usebeamerfont{section title}\insertsection\par
\end{beamercolorbox}
\end{center}
\end{frame}




%--------------------begin took out by JR

% \subsubsection{Divergence and mutual information for arbitrary random variables}
%
% \begin{frame}{Construction of divergence}
% \loud{Partition of a measurable space $(\Omega,\Sigma)$}
% \bit
% \item A \loud{finite partition} $\mathfrak{Q}$ of $\Omega$ is
% a finite set $\mathfrak{Q}=\{A_1,\dots,A_K\}$ of mutually disjoint measurable sets $A_i\in\Sigma$ such that
% \begin{align*}
% \Omega=\sqcup_{i=1}^KA_i.
% \end{align*}
% \item A probability measure $\mu$ on $\Omega$, define a pmf $\mu^{\mathfrak{Q}}$ on $\mathfrak{P}(\mathfrak{Q})$ $\iarrow$ Obtain discrete probabitiliy space as considered previously.
% \item Smallest partition is $\mathfrak{Q}=\{\Omega\}$. Probability space with one point of probabitily one.
% \eit
%
% \loud{Definition of divergence }
% \bit
% \item Let $\mu_1$ and $\mu_2$ be two probability measures on $(\Omega,\Sigma)$.
% \item The divergence $D(\mu_1||\mu_2)$ is defined as
% \begin{align}\label{EqDivergence}
% D(\mu_1||\mu_2):=sup_{\mathfrak{Q}}(D(\mu_1^{\mathfrak{Q}}||\mu_2^{\mathfrak{Q}})),
% \end{align}
% \item Supremumg in \eqref{EqDivergence} is taken over all finite partitions $\mathfrak{Q}$ of $\Omega$.
% \item Supermum in \eqref{EqDivergence}
% can be  infinite.
% \eit
% \end{frame}
%
% \begin{frame} {Divergence. Remarks and definition via special sequences of partitions.}
% Definition of divergence parallel to definition of the integral:
% \bit
% \item Meaning of integral obvious for simple functions. For more general functions, defined via approximation by increasing sequence of simple functions.
% \item Divergence defined for discrete probability spaces. For general proabitliy spaces, define divergence via approximation by increasing sequence of divergences on discrete probabitiliy spaces.
% \eit
%
% Definition \eqref{EqDivergence} very general, involves \textit{all} partitions. \loud{But:} Specific sequence
% of partitions always suffices:
%
% \begin{proposition}
% Let $\mathfrak{Q}_n$ be \textit{any} sequence of finite parititons such that for each $A_{n+1}\in\mathfrak{Q}_{n+1}$ there exists $A_n\in\mathfrak{Q}_n$ with $A_{n+1}\subseteq A_n$ and
% such that $\Sigma$ is generated by $\cup_{n\in\mathbb{N}}\mathfrak{Q}_n$. Then one has
% \begin{align*}
% D(\mu_1||\mu_2)=\lim_{n\to\infty} D(\mu_1^{\mathfrak{Q}_n}||\mu_2^{\mathfrak{Q}_n}).
% \end{align*}
% \end{proposition}
% \loud{Proof:} See book of Gray.
% \end{frame}
%
%
% \begin{frame}{Illustration for the construction of divergence on arbitrary measure spaces}
% \begin{figure}
% \centering
% \includegraphics[width=0.75\textwidth]{RD_V/non_uniform_all_2.png}
% \captionsetup{labelformat=empty}
% \caption{Schematic illustration of the refinement process of a measurable space $\Omega$ into finer and finer partitions $\mathfrak{Q}_n$.
% Set $\Omega$ is uncountable, beige area. Partitions construt collections of finite subsets that are successively refined. Each collection is a finite
% measurable space. Two probability measures on $\Omega$ define two pmfs on each collection. Divergence of the two measures
% is obtained as limit of the divergences on the collections. }
% \end{figure}
% \end{frame}
%
%
% \begin{frame}{Computing divergence for random variables by quantization.}
% \loud{Divergence for random variables}
% \bit
% \item Let $X_1$ and $X_2$ be arbitrary random variables with values in $\mathbb{R}^d$, originating from probability
%  spaces with probability measutes $\mu^1$ and $\mu^2$.
% \item Divergence of $X_1$ and $X_2$ is divergence of their distributions:
% \begin{align*}
% D(X||Y):=D(\mu^1_{X}||\mu^2_{X}).
% \end{align*}
% \item For $N\in\mathbb{N}$, let $Q^N(X_1)$, $Q^N(X_2)$ denote their quantizes versions as in last lecture.
% \item By previous proposition:
% \begin{align}\label{DivRV}
% D(X_1||X_2)=\lim_{N\to\infty} D(Q^N(X_1)||Q^N(X_2)).
% \end{align}
% \item Could also take other sequences of quantizers $\tilde{Q}^N$ satifying assumptions of above proposition.
% \item[] \ALERT{$\iarrow$ Well defined digital-to-analog conversion for divergence of two random variables via
% any quantization scheme  that asymptotically generates the Borel-algebra}.
% \eit
%
%
%
% %
% %\loud{Sequence of partitions $\mathfrak{Q}_n$ for $\mathbb{R}^d$ }
% %\item For $N\in\mathbb{N}$ let $\Delta_N:=2^{-N}$.
% %According to \eqref{UniQuantCells}, for $i=(i_1,\dots,i_n)\in\mathbb{Z}^d$ let
% %\begin{align}\label{UniQuantCells}
% %\mathcal{C}_i^{N}:=\mathcal{C}_i(\Delta_N)= [i_1\cdot\Delta_N,(i_1+1)\cdot\Delta_N)\times\dots\times[i_d\cdot\Delta_N,(i_d+1)\cdot\Delta_N).
% %\end{align}n
% %\item For $N\in\mathbb{N}$ let
% %\[
% %I_N=\{i=(i_1,\dots,i_d)\in\mathbb{Z}^d\colon -N\cdot 2^N\leq i_k< N\cdot 2^N\:\forall k\}.
% %\]
% %\eit
% \end{frame}
%
%
% \begin{frame}{Mutual information}
% \loud{Recall: Mutual information in discrete case}
% \bit
% \item For discrete random-variables $X$ and $Y$ with alphabets $\mathcal{A}_X$ and $\mathcal{A}_Y$ , marginal pmfs $p_X$ and $p_Y$ and joint
% pmf $p_{(X,Y)}$, the mutual information $I(X,Y)$ satisifes
% \begin{align}\label{MutInfDisc}
% I(X,Y)=D(p_{X,Y}||p_X\times p_Y).
% \end{align}
% \item Here $p_X\times p_Y$ is the pmf on $\mathcal{A}_X\times\mathcal{A}_Y$ given as $p_X\times p_Y(x,y)=p_X(x)\cdot p_Y(y)$.
% \item Divergence $D$ now generally defined. \loud{Need to generalize $p_X\times p_Y$.}
% \eit
% \loud{$\iarrow$ Product measure}
% \bit
% \item Let $(\Omega_1,\Sigma_1)$ and $(\Omega_2,\Sigma_2)$ be measure spaces.
% \item \loud{Product $\sigma$-algebra} $\Sigma_1\otimes\Sigma_2$: Smallest $\sigma$-algebra that contains all sets $A\times B$, $A\in\Sigma_1$, $B\in\Sigma_2$.
% \item Let $\mu_1$ measure on $(\Omega_1,\Sigma_1)$ and $\mu_2$
% measure on $(\Omega_2,\Sigma_2)$, both $\sigma$-finite.
% %\item Assume that $\mu_1$ and $\mu_2$ are $\sigma$-finite, i.e. one can write $\Omega_1=\cup_i=1^\infty A_i$ with $A_i\in\Sigma_1$
% %and $\mu_1(\Sigma_1)<\infty$, same for $\mu_2$.
% %\item Example: Probability measures and Lebesgue measure are obviously $\sigma$-finite.
% \item \loud{Product measure:} There exists a unique measure $\mu_1\otimes \mu_2$ on $\Sigma_1\otimes\Sigma_2$ such that
% \begin{align*}
% \mu_1\otimes\mu_2(A\times B)=\mu_1(A)\cdot\mu_2(B),\quad \forall A\in \Sigma_1, \:\forall B\in\Sigma_2.
% \end{align*}
% \eit
% \end{frame}
%
% \begin{frame}{Product measures. Joint distribution. Marginal distribution. Independence.}
% \loud{Lebesgue measure on the Borel algebra as a product measure}
% \bit
% \item For the Borel-algebra, one has
% \begin{align}\label{BorelProduct}
% \mathfrak{B}(\mathbb{R}^{k+l})=\mathfrak{B}(\mathbb{R}^{k})\otimes\mathfrak{B}(\mathbb{R}^l); \quad
% \lambda_{k+l}=\lambda_{k}\otimes\lambda_l.
% \end{align}
% %for the Lebesgue measures \textit{as measures on the Borel algebras}.
% \item For larger $\sigma$-algebra of Lebesgue measurable set, needs to pass to completion.
% \eit
%
% \loud{Joint distribution, marginal distribution, independence:}
% \bit
% \item Let $X:\Omega\to\mathbb{R}^k$ and $Y:\Omega\to\mathbb{R}^l$ be random-variables on a probability spaces $(\Omega,\Sigma,\mu)$.
% \item It follows from \eqref{BorelProduct} that $(X,Y):\Omega\to\mathbb{R}^{k+l}$, $(X,Y)(\omega):=(X(\omega),Y(\omega))$ is a random variable.
% \item As in discrete case, distribution
% \[
% \mu_{X,Y}: \mathfrak{B}(\mathbb{R}^{k+l}) \to[0,1]
% \]
% of $(X,Y)$ is called \loud{joint distribution} of $X$ and $Y$, and
% $\mu_X$ and $\mu_Y$ are called \loud{marginal distributions}.
% \item $X$ and $Y$ are called \loud{independent} if
% \begin{align*}
% \mu_{X,Y}=\mu_X\otimes\mu_Y.
% \end{align*}
% \eit
% \end{frame}
%
% \begin{frame}{Mutual information in the general case}
% Divergence and product measure appearing in \eqref{MutInfDisc}
% are now generalized to the continuous case:
%
% \loud{$\iarrow$ Mutual information can be defined for two variables on a general proability space $(\Omega,\Sigma,\mu)$:}
% \bit
% \item For random variables $X$ and $Y$ on $\Omega$ put
% \begin{align}\label{MutInfCont}
% I(X;Y)=D(\mu_{X,Y}||\mu_X\otimes \mu_Y).
% \end{align}
% \item By \eqref{DivRV}, mutual information between random variables $X$, $Y$ is limit of mutual informations of their
% quantized versions $Q^N(X)$ and $Q^N(Y)$:
% \begin{align}\label{MutContLimit}
% I(X;Y)=\lim_{n\to\infty}I(Q^N(X); Q^N(Y)).
% \end{align}
% \item As for divergence: \ALERT{Any suitable family of quantizers on $\mathbb{R}^d$ gives a well established digital-to-analog conversion for mutual information.}
% \item Using \eqref{MutContLimit}, standard properties of mutual information like symmetry proved for discrete random variables
% carry over to the general case.
% \eit
% \end{frame}
%
%
%
% \begin{frame}{Mutual information via densities}
% \loud{Recall: } Formula for random variable having a \loud{density:}
% \begin{align}\label{EqDiffEntr}
% \lim_{N\to \infty}\left(H(Q^N(X))-dN\right)=h(X)
% \end{align}
% as a motivation for considering
% differences beteween entropies, i.g. mutual informations. Considerations are consistent:
% \begin{proposition}
% Let $(\Omega,\Sigma,\mu)$ be a probability space. Let $X, Y:\Omega\to \mathbb{R}^d$ be absolutely continuous random variables such that $(X,Y)$ is absolutely continuous. Let $f_X$ and
% $f_Y$ denote the densities of $X$ and $Y$ and let $f_{X,Y}$ denote the density of $(X,Y)$. Then one has
% \begin{align}\label{FormulaMIIntegral}
% I(X;Y)=\int_{\mathbb{R}}\int_{\mathbb{R}}f_{X,Y}(x,y)\log_2\left(\frac{f_{X,Y}(x,y)}{f_X(x)f_Y(y)}\right)dxdy.
% \end{align}
% \end{proposition}
% \vspace{-0.2cm}
% \loud{Proof}
% \vspace{-0.1cm}
% \bit
% \item General case: Use Radon-Nikodym theore, see book of Gray.
% \item Here:  Assume that \eqref{EqDiffEntr} can be applied.
% \eit
% \end{frame}
%
% \begin{frame}{Mutual information via densities. Proof.}
% \bit
% \item One has
% \begin{align*}
% I(X;Y)=&\lim_{N\to\infty}I(Q^{N}(X);Q^N(Y))\\
% =&\lim_{N\to\infty}\left(H(Q^{N}(X,Y))-H(Q^{N}(X))-H(Q^{N}(Y))\right)\\
% =&\lim_{N\to\infty}\left(H(Q^{N}(X,Y))-2dN)-(H(q^{N}(X))-dN)-(H(q^{N}(Y))-dN)\right).
% \end{align*}
% \item
% Thus by \eqref{EqDiffEntr}, one has
% \begin{align*}
% I(X;Y)
% =\int_{\mathbb{R}}\int_{\mathbb{R}}f_{X,Y}(x,y)\log_2(f_{X,Y}(x,y))dxdy-
% \int_{\mathbb{R}}f(x)\log_2(f(x))dx-\int_{\mathbb{R}}f(y)\log_2(f(y))dy.
% \end{align*}
% \item For the marignal denities $f_X$ and $f_Y$, one has
% \begin{align*}
% f_X(x)=\int_{\mathbb{R}}f_{X,Y}(x,y)dy;\quad f_X(y)=\int_{\mathbb{R}}f_{X,Y}(x,y)dx,\quad\text{allmost everywhere.}\qed
% \end{align*}
% \eit
% Similarly: \loud{Definition of divergences are consistent} if $X$ and $Y$ have densities.
% \end{frame}
%
%
% \begin{frame}{Informational rate-distortion function, general case}
% Mutual information defined $\iarrow$ Can define informational rate distortion function as before.
% \begin{definition}
% Let $(\Omega,\Sigma,\mu)$ be a probability space and let $X:\Omega\to\mathbb{R}$ be a continuous
% random variable (r.v.). Let $d$ be a distortion function. The informational rate distortion function is defined as
% \begin{align*}
% R^{(I)}(D):=\inf\bigl\{&I(X;\hat{X})\colon  \text{$\hat{X}$ is an $\mathbb{R}$-valued r. v.  and there exists an $\mathbb{R}^2$-valued r. v. $Z=(Z_1,Z_2)$}\\ &\text{on some probability space with probability measure $\nu$ such that $Z_1\stackrel{d}{=} X$ and $Z_2\stackrel{d}{=} \hat{X}$} \\ &\text{and such that
% $\int_{\mathbb{R}^2}d(x,\hat{x})d\nu_Z(x,\hat{x})<D$. }
% \bigr\}
% \end{align*}
% \end{definition}
% \vspace{-0.2cm}
% \bit
% \item Recall: $X\stackrel{d}{=} Y$ for r.v. $X$ and $Y$ means that $X$ and $Y$ have the same distribution.
% \item Definition is consistent with previous definition of $R^{(I)}(D)$ for the discrete case.
% \item As in discrete case, can also define $R^{(I)}(D)$ using conditional distribution instead of $Z$.
% \item However: (Regular versions of) conditional expectation not introduced in this lecture.
% %\item See standard textbooks on probability theory.
% %\item Suffices to work with joint distribution $Z$.
% \eit
% \end{frame}
%--------------------end took out by JR

\subsection{Source codes and rate-distortion functions for general random variables.}
\begin{frame}
 \vspace{12.0ex}
\begin{center}
\begin{beamercolorbox}[sep=12pt,center]{part title}
\usebeamerfont{section title}\insertsubsection\par
\end{beamercolorbox}
\end{center}
\end{frame}

\begin{frame}{Lossy source codes for general sources $X$} 
Let $X$ be a random-variable with values in $\mathbb{R}^d$ and distribution $\mu_X$. 
%\vspace{-0.2cm}
\loud{Lossy source code $Q=(\alpha,\gamma,\beta)$ for $X$}
\bit
\item Same definition as before. But: Quantization defined as mapping on the whole $\mathbb{R}^d$.
\item $\alpha: \mathbb{R}^d\to \mathcal{I}_K:=\{1,\dots,K\}$, \loud{quantization}
\item $\gamma$: \loud{Lossless mapping}. Uniquely decodable code for $\mathcal{I}_K$. 
\item $\beta: \mathcal{I}_K\to\mathbb{R}^d$, \loud{inverse quantization}. 
\eit

\loud{Main difference to case of finite alphabets:} 
\bit
\item One never has: $\beta(\alpha(x))\neq x$.
\item[\iarrow] If one assumes that $X$ does not attain only finitely many values $\mathcal{A}_X$,  there is always loss in information.
\item Lossless source codes do not exist for general random variables.
\eit
\vspace{-0.2cm}
\loud{Rate of a source code:}
\bit
\item Defined as before: $\mu_X$ and $\alpha$ induce discrete probability distribution on $\mathcal{I}_K$. 
\item [\iarrow] Rate is average expected codeword-length of $\gamma$. 
\eit


\end{frame}

\begin{frame}\frametitle{Lossy source codes: Distortion}
\bit
\item Fix a distortion measure $d$ on $\mathbb{R}$, get additive extension $d_N$ on $\mathbb{R}^N$. 
\item The distortion $\delta(Q)$ of a source code $Q$ is defined as the expected mean squared Euclidean distance between original and 
reconstructed symbols  
\begin{align*}
\delta(Q)=\int_{\mathbb{R}^N}d_N(s,\beta(\alpha(s)))d\mu_X(s).
\end{align*}
\item Define a discrete random variable $Y$ on $\Omega$ by $Y(\omega):=\beta(\alpha(X(\omega)))$, quantization of $X$.
\item Let $Z:=(X,Y)$. $Z$ is a random variable; never absolutely continuous. 
\item Distribution $\mu_{Z}$ of $Z$ is a probability measure on $\mathbb{R}^{N}\times\mathbb{R}^N$.
%If $X$ is continous, $Z$ is a mixture of 
%a continuous and discrete random variable, does not have a density.
\item By definition, one has
\begin{align*}
\delta(Q)=\int_{\mathbb{R}^N\times\mathbb{R}^N}d_N(x,\hat{x})d\mu_Z(x,\hat{x}).
\end{align*}
\item For $N=1$, last equation is consistent with distortion term in informational rate distortion function. 
\eit 
\end{frame}


\begin{frame}\frametitle{Lossy source codes: Quantization cells}
Let $Q=(\alpha,\gamma,\beta)$ be a source code. 
\begin{itemize}
\item For $i\in\mathcal{I}_K$, the set 
$C_i:=\alpha^{-1}(i) $
is called the \loud{quantization cell} of index $i$.
\item The value 
$\hat{x}_i:=\beta(i)$ is called the reconstructed value of index $i$. 
\item The quantization cells $C_i$ are pairwise disjoint and their union is $\mathbb{R}^N$. 
\item One has 
\begin{align}\label{RDQuantCells}
\delta(Q)=\sum_{i\in\mathcal{I}_K}\int_{C_i}\left\|x-\hat{x}_i\right\|^2d\mu_X(x),\quad r(Q)=\sum_{i\in\mathcal{I}_K}|\gamma(i)|\mu_X(C_i).
\end{align}
\begin{figure}
\centering
\includegraphics[width=0.12\textwidth]{RD_V/quant_cells.png}
\captionsetup{labelformat=empty}
\caption{Example for quantization cells and reconstruction points in $\mathbb{R}^2$.}
\end{figure}
\end{itemize}
\end{frame}




\begin{frame}{Rate distortion function and fundamental source coding theorem. General case.}  
\loud{Rate distortion function}
\bit
\item Let $X$ be an $\mathbb{R}^N$-valued random variable and let $Q$ be a source code for $X$.
\item For $D\in [0,\infty)$, one puts
\begin{align*}
R(D)= \inf\{r(Q)\colon \text{$Q$ is a source code with $\delta_N(Q)\leq D$}\}.
\end{align*}
\item If support of $\mu_X$, i.e. $\{x\in\mathbb{R}^N\colon \mu_X(x)\neq 0$\} is uncountalbe, $R(D)$ goes to infinity as $D\to 0$.
\item In particular: $R(D)$ goes to infinity as $D\to 0$ if $X$ is an absolutely continuous random variable.
\eit

\loud{Setup for fundamental lossless source coding theorem:} Analog to discrete case. 
\bit
\item Let $X$ be an $\mathbb{R}$-valued random variable. 
\item Consider joint coding of $N$ independent realizations of $X$, i.e. 
joint coding of $\mathbb{R}^N$-valued sources $X^N=(X_1,\dots,X_N)$ with 
process $iid$, $X_i\stackrel{d}{=}X$ for all $i$.
\item If $Q$ is a source code for $X^N$, rate $r(Q)$ is meant as rate per symbol, i. e. normalized with $1/N$.
\item Fix a distortion function $d$ on $\mathbb{R}$, extend $d$ to additive distortion function on $\mathbb{R}^N$.  
\eit 
\end{frame}

\begin{frame}{Fundamental source coding theorem for general random variables.}
\begin{theorem} 
\begin{enumerate}
\item The informational rate distortion function $R^{(I)}$ is always a \loud{lower bound} for the rate distortion function: 
For every $N\in\mathbb{N}$ and every source code $Q_N$ for $X^N$ one has
\begin{align}\label{BoundRate}
r(Q_N)\geq R^{(I)}(\delta(Q_N)). 
\end{align}
\item The bound \eqref{BoundRate} is \loud{asymptotically achievable}: For every $D>0$, every $\epsilon>0$ and every $R'>R^{(I)}(D)$, one can achieve 
\begin{align*}
\delta(Q_N)\leq D+\epsilon, \quad r(Q_N)\leq R'
\end{align*}
for some sequence $Q_N$  of source codes for $X^N$ where $N$ can be chosen arbitrarily large.    
\end{enumerate} 
\end{theorem}
\loud{Proof: Proceed exactly as in discrete case.}
\bit
\item Mutual information has same properties as in discrete case.% by \eqref{MutContLimit}.
\item Law of large number holds for $X^N$ $\iarrow$ Ensemble set and random-coding can be constructed as in the discrete setting. 
\eit 
\end{frame}


\subsection{Rate distortion function for a scalar Gaussian source}

\begin{frame}
 \vspace{12.0ex}
\begin{center}
\begin{beamercolorbox}[sep=12pt,center]{part title}
\usebeamerfont{section title}\insertsubsection\par
\end{beamercolorbox}
\end{center}
\end{frame}



\subsubsection{Basic properties of the Gaussian Distribution}

\begin{frame}{Review: Definition of the scalar Gaussian distribution}
\loud{The Gaussian function} 
\bit
\item For $\sigma\in(0,\infty)$, the \loud{Gaussian function} is defined as 
\begin{align*}
\phi_{\sigma}(x):=\frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{x^2}{2\sigma^2}\right).
\end{align*}
%\item For any $\mu\in\mathbb{R}$, one has 
%\begin{align*}
%\int_{\mathbb{R}}\phi_\sigma(x-\mu)dx=1;\quad \int_{\mathbb{R}}\phi_\sigma(x-\mu)xdx=\mu;\quad \int_{\mathbb{R}}\phi_\sigma(x-\mu)(x-\mu)^2dx=\sigma^2.
%\end{align*}
\item Integration of $\phi_{\sigma}(x-\mu)$ defines a probability measure on $\mathfrak{B}(\mathbb{R})$ with mean $\mu$ and variance $\sigma^2$. 
\eit

\loud{Gaussian distributed random variables}
\bit
\item An $\mathbb{R}$-valued random variable $X$ is \loud{Gaussian distributed with mean $\mu$ and variance $\sigma$}, 
\begin{align*}
X\sim \mathcal{N}(\mu,\sigma^2), 
\end{align*}
if 
$X$ is absolutely continuous and its density $f_X$ satisfies $f_X(x)=\phi_{\sigma}(x-\mu)$ almost everywhere.
\item Gaussian distribution arguably \loud{most important model distribution}. 
\item One important  reason: \loud{Central limit theorem}. 
\eit
\end{frame}









\begin{frame}
\begin{proposition}[Differential entropy for the Gaussian source.]
\begin{enumerate}
\item The differential entropy of a normally distributed random variable $X\sim N(\mu,\sigma^2)$ satisfies
\begin{align}\label{EqDiffEntrGV}
h(X)= \frac{1}{2}\log_2(2\pi\sigma^2 e).
\end{align}
\item All absolutely continuous sources $Y$ having a finite differential entropy and having finite variance $\sigma^2$ satisfiy
\begin{align}\label{IneqEntr}
h(Y)\leq \frac{1}{2}\log_2(2\pi\sigma^2 e).
\end{align}
The Gaussian distribution $\mathcal{N}(\mu,\sigma^2)$ maximes the differential entropy for a given variance. 
\end{enumerate}
\end{proposition}
\bit
\item Using \loud{Shannon lower bound}: Will be shown that Gaussian iid process is `most difficult to code for high rates' among 
all iid processes with same variance.
\item [\iarrow] Heuristically consistent with maximalization \eqref{IneqEntr} and meaning of entropy from discrete case.  
\eit

\end{frame}

\begin{frame}{Computation of differential entropy of scalar Gaussian source.}
\bit
\item For $X\stackrel{d}{=} N(\mu,\sigma^2)$ one computes
\begin{align*}
h(X)=&-\int_{\mathbb{R}}\phi_{\sigma}(x-\mu)\log_2(\phi_{\sigma}(x-\mu))dx\\
=&-\int_{\mathbb{R}}\phi_{\sigma}(x)\log_2(\phi_{\sigma}(x))dx\\
=& \frac{1}{\ln(2)}\cdot\frac{1}{\sqrt{2\pi\sigma^2}}\int_{\mathbb{R}}\exp\left(-\frac{x^2}{2\sigma^2}\right)\left(\ln(\sqrt{2\pi\sigma^2})+\frac{x^2}{2\sigma^2}\right)dx\\
=&\frac{1}{\ln(2)}\cdot\left(\ln(\sqrt{2\pi\sigma^2})\int_{\mathbb{R}}\phi_\sigma(x)dx+\frac{1}{2\sigma^2}\int_{\mathbb{R}}\phi_\sigma(x)x^2dx\right)\\
=&\frac{1}{\ln(2)}\cdot\left(\ln(\sqrt{2\pi\sigma^2})+\frac{1}{2}\right)\\
=&\frac{1}{2\ln(2)}\cdot\left(\ln(2\pi\sigma^2)+1\right)\\ 
=&\frac{1}{2}\log_2(2\pi\sigma^2 e).
\end{align*}
\eit
\end{frame} 

\begin{frame}{Scalar Gausian distribution maximes differential entropy}
\loud{Proof}
\bit
\item Let $Y$ be any absolutely continous distribution of variance $\sigma^2$ and probability density function $g$.
\item One has 
\begin{align*}
0\leq D(g||\phi_\sigma)=&\int_{\mathbb{R}}g(x)\log_2\left(g(x)/\phi_\sigma(x)\right)dx\\
=&\int_{\mathbb{R}}g(x)\log_2(g(x))dx-\int_{\mathbb{R}}g(x)\log_2(\phi_\sigma(x))dx\\
=&-h(g)+\frac{1}{\ln(2)}\int_{\mathbb{R}}g(x)\left(\ln(\sqrt{2\pi\sigma^2})+\frac{x^2}{2\sigma^2}\right)dx\\
=&-h(g)+\frac{1}{\ln(2)}(\ln(\sqrt{2\pi\sigma^2})\int_{\mathbb{R}}g(x)dx+\frac{1}{2\sigma^2}\int_{\mathbb{R}}x^2g(x)dx)\\
=&-h(g)+\frac{1}{\ln(2)}(\ln(\sqrt{2\pi\sigma^2})+1/2)\\
=&-h(g)+h(\mathcal{N}(0,\sigma^2)).
\end{align*}
\eit
\end{frame}

\subsubsection{Rate distortion function for scalar Gaussian sources}
\begin{frame}{Rate-distortion function of a Gaussian source}
\begin{theorem}
Let $X$ be an $\mathbb{R}$-valued Gaussian source of mean $\mu$ and variance $\sigma^2$, i.e. $X\stackrel{d}{=} \mathcal{N}(\mu,\sigma^2)$. Then one has 
\begin{align}\label{RDGauss}
R(D)=\begin{cases} &0, \text{if $D\geq \sigma^2$} \\ &\frac{1}{2}\log_2\left(\frac{\sigma^2}{D}\right), \text{else.} \end{cases}
\end{align}
\end{theorem}
\bit
\item [\iarrow] \loud{Explicit formula for the rate-distortion function of a very important source, probably most 
important model source!}
\eit
\vspace{-0.2cm}
\loud{Many application of \eqref{RDGauss} and its extensions will be given:} 
\bit
\item Can explicitly quantify vector quantization advantage for Gaussian sources.
\item Can explicity compare optimal transform coding 
scheme with general vector quantization for Gaussian sources. 
\item Transform coding: Cornerstone of modern image- and video-codecs.
 \eit
\end{frame}

\begin{frame}{Rate-distortion function of a scalar Gaussian source}
\loud{Lower bound for $R^{(I)}(D)$: }
\bit
\item Let $\hat{X}$ be a random-variable that joint distribution of $X$ and $\hat{X}$ satisfies the distortion constraint. Then: 
\begin{align}
I(X;\hat{X})=h(X)-h(X|\hat{X})
= & h(X)-h(X-\hat{X}|\hat{X})\nonumber\\
\geq & h(X)- h(X-\hat{X})\nonumber\\
\geq & h(X) - \sup_{\{Y\colon var(Y)<D\}}h(Y)\nonumber\\
\geq & h(X) - \frac{1}{2}\log_2(2\pi\sigma^2 e)\label{GaussRDI}\\
=& \frac{1}{2}\log_2\left(\frac{\sigma^2}{D}\right)\label{GaussRDII}. 
\end{align}
\item Here, \eqref{GaussRDI} follows from \eqref{IneqEntr} and \eqref{GaussRDII} follows from \eqref{EqDiffEntrGV}. 
\item Justsification of other (in)equalities. See below. 
\eit

\end{frame}




\begin{frame}{Rate-distortion function of a 1-D Gaussian source.}
\loud{Proof of achievability.}
\bit
\item Let $\hat{X}$ be a Gaussian source such that
\bit
\item $\hat{X}\stackrel{d}{=} \mathcal{N}(0,\sigma^2-D)$.
\item $X-\hat{X}\stackrel{d}{=} \mathcal{N}(0,D)$.
\item $\hat{X}$ and $X-\hat{X}$ are independent.
\eit
\item \textbf{Exercise: } Show that $\hat{X}$ exists. Use that convolution of Gaussians is Gaussian, mean and variance add up. 
\item One has
\begin{align*}
I(X;\hat{X})=&h(X)-h(X|\hat{X})\\
=&h(X)-h(X-\hat{X}|\hat{X})\\
=&h(X)-h(X-\hat{X})\\
=& \frac{1}{2}\log_2\left(\frac{\sigma^2}{D}\right).
\end{align*}
\eit
\end{frame}

\begin{frame}{Rate distortion function of scalar Gaussian source. Remarks on proof.}
\loud{Remark on the proof: }
\bit
\item  Previous (in)equalities are easily verified for \loud{discrete case}.
\item Using \eqref{MutContLimit},  they extend easily to present case. 
\item One does not even need to define conditional entropy for continuous case.
\item  Argue exactly as in the proof of \eqref{FormulaMIIntegral}, using that \eqref{EqDiffEntr} also holds for the Gaussian distribution. 
\eit
\end{frame}

\begin{frame}{Rate-distortion function of a Gaussian source: Vector quantization advantage}
Let $X$ be a 1-D Gaussian source.
\bit
\item \loud{Scalar quantization:} Rate-distortion function of $X$ or of $X^N$ when coding each component of $X^N$ separately (i.e. concatenation of 1-D source-codes).  
\item Let $R_{sq}(D)$ be rate-distortion function for scalar quantization of $X$. 
\item After Christmas, \loud{High rate approximation of Gish and Pierce: }:
One has
\[
R_{sq}(D)\approx \frac{1}{2}\log_2\left(\frac{\pi e}{6}\cdot\frac{\sigma^2}{D}\right), \quad\text { as $R\to\infty$.}
\]
\item[\iarrow] Compare to rate-distortion function of $X$:  
\begin{align}\label{VQ_Advantage}
R_{sq}(D)-R(D)\approx \frac{1}{2}\log_2\left(\frac{\pi e}{6}\right)\approx 1/4,  \quad\text { as $R\to\infty$.}
\end{align}
\eit
\alert{\iarrow \textbf{Vector quantization advantage of 1/4 bit per sample for Gaussian source at high rates.}}
\bit
\item Equation \eqref{VQ_Advantage} holds for more general sources by \textit{asymptotic tightness of Shannon lower bound}. 
\eit
\end{frame}

\end{document}
